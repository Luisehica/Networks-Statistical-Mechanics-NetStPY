{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Funciones Proyecto.ipynb","provenance":[],"authorship_tag":"ABX9TyMU/RcUFkPN0s2z9TTmJVcg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"W8Rg9AwqWe3Y"},"source":["#networkx para teoría de grafos, matplot para gráficar, random para generar números aleatorios, stats para la entropia y solve para optimizar.\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy as sp \n","from scipy import stats as st\n","import math \n","from scipy.optimize import fsolve"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Y751YjMWr0t"},"source":["# Esta función genera un vector con probabilidades acumuladas de la distribución de pareto\n","def pareto_cumm_probabilities(particiones,xfin, distribution = st.pareto.cdf):\n","  \"\"\"\n","  (particiones,xfin) -----> (probability_cum_vector)\n","  Esta función recibe un xfinal hasta donde sumar y un número de particiones\n","  juntos definen la longitud y el valor máximo del vector resultante\n","  \"\"\"\n","  dx = xfin/particiones\n","  x = []\n","  probability_cum_vector = []\n","  for i in range(particiones):\n","    equis = 1 + dx*i\n","    x.append(equis)\n","    pes = st.pareto.cdf(x[i],2.4) #se eligió un coeficiente de 2.4 para la distribución de pareto\n","    probability_cum_vector.append(pes)\n","  return probability_cum_vector, x # entrega como resultado el vector y los valores de x asociados"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjGEK5Mvel4k"},"source":["#La siguiente función nuestrea secuencias de números con la distribución del vector de probabilidades acumuladas\n","def Degree_Sec_Generator(Probabilidad_Acumulada,longitud_Secuencia):\n","  \"\"\"\n","  (Vector Probabilidada Acumulada, Longitud Secuencia) -------> Secuencia de enteros con la distribución del vector\n","  \"\"\"\n","  Degree_Sequence = []\n","  for i in range(longitud_Secuencia):\n","    k = st.uniform.rvs(scale = Probabilidad_Acumulada[len(Probabilidad_Acumulada)-1]) #Se generan números aleatoriamente entre 0 y 0.9999\n","    for j in range(len(Probabilidad_Acumulada)-2):\n","      if k <= Probabilidad_Acumulada[j+2]: # Asocia un entero positivo a cada número entre 0-0.9999\n","        Degree_Sequence.append(j+2) #construye el vector con los enteros positivos asociados a los números entre 0-1\n","        break\n","  return Degree_Sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SJQB2wSUXMJD"},"source":["#Este algoritmo muestrea secuencias de GRADO con la distribución del vector de probabilidades acumuladas, entrega como output secuencia de grado promedio\n","def Deg_Sec_Prom(Muestreo_seq,Number_nodes,Probability_distribution):\n","  \"\"\"\n","  (num_muestras,number_nodes,cum_prob_distribution) ----------------> (degree_sequence_prom)\n","  esta función hace uso de Degree_Sec_Generator y networkx\n","  \"\"\"\n","  degree_sequences = [] #matriz donde se guardaran las distintas secuencias a muestrear\n","  for i in range(Muestreo_seq): #por cada i generamos una secuencia\n","    degree_sequences.append([]) \n","    for k in range(10000):\n","      Degree_Sequence = Degree_Sec_Generator(Probability_distribution,Number_nodes) #generamos la secuencia de números\n","      if nx.is_valid_degree_sequence_havel_hakimi(Degree_Sequence) == True:\n","        if nx.is_connected(nx.havel_hakimi_graph(Degree_Sequence)) == True: #validamos que sea apta para generar un grafo simple\n","          Degree_Sequence.sort() #la ordenamos, esto con el fin de darle una identidad a los nodos acorde a su puesto en el ranking de medida\n","          for l in range(len(Degree_Sequence)): #este for es para agregar la secuencia de grado a la matriz creada inicialmente\n","            degree_sequences[i].append(Degree_Sequence[l])\n","          break\n","\n","  Degree_sequence_prom = [] #esta parte del codigo calcular el promedio\n","  for i in range(Number_nodes):\n","    sum = 0\n","    for j in range(Muestreo_seq):\n","      sum = sum + degree_sequences[j][i]\n","    Degree_sequence_prom.append(sum/Muestreo_seq)\n","  return Degree_sequence_prom"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlZqlDDUXUfs"},"source":["# este programa te calcula los multiplicadores de lagrange dada una secuencia de grado acorde al ensamble canonico.\n","def multp_lagrange_canonico(degree_sequence):\n","  \"\"\"\n","  (degree_sequence) ----> (lagrange_multiplicators_canonicalensemble)\n","  esta función te recibe una secuencia de grado, te la impone como ligadura en un ensamble canonico de grafos simples y \n","  te entrega un vector con los multiplicadores de lagrange ordenados de menor a mayor acorde al grado de la restricción asociada \n","  (para más información ver soft configuration model y Newm)\n","  \"\"\"\n","  def lagrange_eq(lagrange_multiplicators, degree_sequence):\n","    n = len(degree_sequence)\n","    matrix = np.zeros((n,n))\n","    for i in range(n):\n","      for j in range(n):\n","        matrix[i][j] = 1/(math.exp(lagrange_multiplicators[i] + lagrange_multiplicators[j]) + 1)\n","      matrix[i][i] = 0\n","    sums = []\n","    for i in range(n):\n","      sum = 0\n","      for j in range(n):\n","        sum = sum + matrix[i][j]\n","      sums.append(sum)\n","    resultado = np.zeros(n)\n","    for i in range(n):\n","      resultado[i] = sums[i] - degree_sequence[i] \n","    return resultado\n","\n","  x0 = np.zeros(len(degree_sequence))\n","  def lagrange(lagrange_multiplicators):\n","    k = lagrange_eq(lagrange_multiplicators,degree_sequence)\n","    return k\n","  #hasta acá lo que se ha hecho es definir las funciones a resolver acorde a los resultados del ensamble canonico \n","  #(para más información buscar soft configuration model)\n","  multiplicators = fsolve(lagrange,x0) #se resuelve el conjunto de ecuaciones hallando los multiplicadores\n","  return multiplicators"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZUA9EruXbJY"},"source":["#Esta función recibe un vector con los multiplicadores de lagrange (ordenados de tal manera que el grado correspondiente como ligadura esté ordenado de menor a mayor)\n","def canonical_matrix(Multiplicadores_Lagrange):\n","  Adyacencia_promedio = []\n","  for i in range(len(Multiplicadores_Lagrange)):\n","    Adyacencia_promedio.append([])\n","    for j in range(len(Multiplicadores_Lagrange)):\n","      Adyacencia_promedio[i].append(1/(1 + math.exp((Multiplicadores_Lagrange[i] + Multiplicadores_Lagrange[j]))))\n","  for i in range(len(Multiplicadores_Lagrange)):\n","    Adyacencia_promedio[i][i] = 0\n","  return Adyacencia_promedio #entrega la matriz de adyacencia promedio según el ensamble canónico "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCZquQjpX2z7"},"source":["#esta función recibe vector de probabilidades acumulada y un entero, te entrega un grafo generado con el algoritmo expuesto en el proyecto que sustenta este respositorio\n","def maxent_generator(cum_probability,number_nodes,m):\n","  \"\"\"\n","  (cum_probability,number_nodes) -----> G\n","  esta función recibe vector de probabilidades acumulada y un entero, te entrega un grafo generado con el algoritmo expuesto\n","  en el proyecto que sustenta este respositorio\n","  \"\"\"\n","  for k in range(1000000000000):\n","          Degree_Sequence = Degree_Sec_Generator(cum_probability,number_nodes,m) #se genera secuencia\n","          if nx.is_valid_degree_sequence_havel_hakimi(Degree_Sequence) == True:\n","            if nx.is_connected(nx.havel_hakimi_graph(Degree_Sequence)) == True: #se comprueba si permite crear un grafo simple conectado\n","              Degree_Sequence.sort() #ordenamos la secuencia\n","              Grafo = nx.havel_hakimi_graph(Degree_Sequence) #creamos el grafo con el algoritmo havel hakimi\n","              break\n","  nx.double_edge_swap(Grafo,nswap=1000,max_tries=150000) #Se aleatoriza el grafo manteniendo su secuencia de grado constante\n","  return Grafo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhSOdjcSX9Qr"},"source":["#Estas funciones remueven N nodos o enlaces del grafo ingresado\n","def remove_hubs_load(G,nodos_removidos): #remueve nodos con mayor load\n","  \"\"\"\n","  (G, #nodos_removidos) ---------> G\n","  \"\"\"\n","  for i in range(nodos_removidos):\n","    keys = list(nx.load_centrality(G).keys())\n","    values = list(nx.load_centrality(G).values())\n","    maxval = max(values)\n","    casilla_nodo = values.index(maxval)\n","    G.remove_node(keys[casilla_nodo])\n","  return G\n","\n","def edge_remove_hub_load(G,edges_removidos): #remueve edges con mayor load\n","  \"\"\"\n","  (G,#edges_removidos) -------> G\n","  \"\"\"\n","  for i in range(edges_removidos):\n","    keys = list(nx.edge_load_centrality(G).keys())\n","    values = list(nx.load_centrality(G).values())\n","    maxval = max(values)\n","    casilla_edge = values.index(maxval)\n","    G.remove_edge(keys[casilla_edge][0],keys[casilla_edge][1])\n","  return G\n","\n","\n","def remove_aleatory(G,nodos_removidos): #remueve nodos aleatoriamente\n","  \"\"\"\n","  (G,#nodos_removidos) -----------> G\n","  \"\"\"\n","  keys = list(G.nodes())\n","  for i in range(nodos_removidos):\n","    remove_node = np.random.randint(0,G.number_of_nodes())\n","    remnode = keys[remove_node]\n","    if (remnode in G) == True:\n","      G.remove_node(remnode)\n","  return G\n","\n","def edge_remove_aleatory(G,edges_removidos): #remueve edges aleatoriamente\n","  \"\"\"\n","  (G,#edges_removidos) ------> G\n","  \"\"\"\n","  for i in range(edges_removidos):\n","    remove_edge = np.random.randint(0,len(nx.edges(G)))\n","    if (remove_edge in G) ==True:\n","      G.remove_edge(list(nx.edges(G))[remove_edge][0],list(nx.edges(G))[remove_edge][1])\n","  return G"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phohpFXwonIe"},"source":["def hub_cascade_failure(G,Initial_Capacity,Resiliencia,Number_Attacks):\n","  \"\"\"\n","  (Graph,float,float,int) -------> (G)\n","  Esta función ataca nodos acorde a su ranking en la medida load y luego aplica una falla en cascada\n","  acorde a que nodos soportan un mayor load que el dado por su capacidad inicial y resiliencia\n","  \"\"\"\n","  load1 = nx.load_centrality(G)\n","  keys1 = list(load1.keys())\n","  nodes = len(G.nodes())\n","  Capacity = []\n","  for i in range(len(keys1)): #se definen condiciones iniciales\n","    q = (1+Initial_Capacity)*load1[keys1[i]]\n","    Capacity.append(q)\n","  DAMAGE = []\n","  ATTACK = []\n","  for i in range(Number_Attacks): #en este for se ejecutan los ataques\n","    NodosBC = len(max(nx.connected_components(G), key=len))\n","    DAMAGE.append(NodosBC/nodes) #registramos el daño con el cambio en el tamaño de la componente principal\n","    ATTACK.append(i) #registramos el número de ataque\n","    remove_hubs_load(G,1) #se remueve el nodo con mayor load\n","    DELETE_NODES = []\n","    load = nx.load_centrality(G)\n","    for j in G.nodes():\n","      val = Resiliencia*Capacity[keys1.index(j)] #calculamos el valor de load sobre el cual un nodo colapsara con 100% de seguridad\n","      if load[j] > val: \n","        DELETE_NODES.append(j) #enlistamos los nodos que colapsaran\n","      else:\n","        if load[j] > Capacity[keys1.index(j)]:#se elige si un nodo que ha superado su capacidad colapsa\n","          k = st.uniform.rvs()\n","          P = (1/(Resiliencia-1))*(load[j]/Capacity[keys1.index(j)] -1)\n","          if k <= P:\n","            DELETE_NODES.append(j) #enlistamos los nodos colapsados\n","    G.remove_nodes_from(DELETE_NODES) #se eliminan los nodos colapsados\n","\n","  return G,ATTACK,DAMAGE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i98mvQtUonOb"},"source":["def edge_hub_cascade_failure(G,Initial_Capacity,Resiliencia,Number_Attacks):\n","  \"\"\"\n","  (Graph,float,float,int) -------> (G)\n","  Esta función ataca nodos acorde a su ranking en la medida load y luego aplica una falla en cascada\n","  acorde a que nodos soportan un mayor load que el dado por su capacidad inicial y resiliencia\n","  \"\"\"\n","  load1 = nx.edge_load_centrality(G)\n","  keys1 = list(load1.keys())\n","  Capacity = []\n","  for i in range(len(keys1)): #se definen condiciones iniciales\n","    q = (1+Initial_Capacity)*load1[keys1[i]]\n","    Capacity.append(q)\n","  DAMAGEEDGE = []\n","  ATTACK = []\n","  for i in range(Number_Attacks): #en este for se ejecutan los ataques\n","    NodosBC = len(max(nx.connected_components(G), key = len))\n","    DAMAGEEDGE.append(NodosBC/len(G.nodes()))#registramos el daño con el cambio en el tamaño de la componente principal\n","    ATTACK.append(i) #registramos el número de ataque\n","    edge_remove_hub_load(G,1) #se remueve el edge con mas load \n","    DELETE_EDGES = []\n","    load = nx.edge_load_centrality(G)\n","    for j in G.edges():\n","      val = Resiliencia*Capacity[keys1.index(j)] #calculamos el valor de load sobre el cual un nodo colapsara con 100% de seguridad\n","      if load[j] > val:  #enlistamos los nodos que colapsaran\n","        DELETE_EDGES.append(j)\n","      else:\n","        if load[j] > Capacity[keys1.index(j)]: #se elige si un nodo que ha superado su capacidad colapsa\n","          k = st.uniform.rvs()\n","          P = (1/(Resiliencia -1))*(load[j]/Capacity[keys1.index(j)] - 1)\n","          if k <= P:\n","            DELETE_EDGES.append(j) #enlistamos los nodos colapsados\n","    G.remove_edges_from(DELETE_EDGES) #se eliminan los nodos colapsados\n","    \n","  return G, ATTACK, DAMAGEEDGE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0S-jsdOfEFc"},"source":["def aleatory_cascade_failure(G,Initial_Capacity,Resiliencia,Number_Attacks):\n","  \"\"\"\n","  (Graph,float,float,int) -------> (G,Vector con los daños, vector con el número de ataques)\n","  Esta función luego de atacar un nodo aleatoriamente ejecuta el algoritmo de falla en cascada y te entrega como resultado\n","  el grafo atacado, un vector con los daños por ataque y un vector con el número de ataques\n","  \"\"\"\n","  load1 = nx.load_centrality(G)\n","  keys1 = list(load1.keys())\n","  Capacity = []\n","  for i in range(len(keys1)): #se definen condiciones iniciales\n","    q = (1+Initial_Capacity)*load1[keys1[i]]\n","    Capacity.append(q)\n","  DAMAGE = []\n","  ATTACK = []\n","  for i in range(Number_Attacks): #en este for se ejecutan los ataques\n","    NodosBC = len(max(nx.connected_components(G), key = len))\n","    DAMAGE.append(NodosBC/len(G.nodes()))#registramos el daño con el cambio en el tamaño de la componente principal\n","    ATTACK.append(i) #registramos el número de ataque\n","    remove_aleatory(G,1) #se remueve un nodo aleatoriamente\n","    DELETE_NODES = []\n","    load = nx.load_centrality(G)\n","    for j in G.nodes():\n","      val = Resiliencia*Capacity[keys1.index(j)] #calculamos el valor de load sobre el cual el nodo colapsara con 100% de probabilidad\n","      if load[j] > val: \n","        DELETE_NODES.append(j) #enlistamos los nodos colapsados\n","      else:\n","        if load[j] > Capacity[keys1.index(j)]: # se decide si un nodo que ha superado su capacidad colapsa\n","          k = st.uniform.rvs()\n","          P = (1/(Resiliencia-1))*(load[j]/Capacity[keys1.index(j)] -1)\n","          if k <= P:\n","            DELETE_NODES.append(j) #enlistamos los nodos colapsados\n","    G.remove_nodes_from(DELETE_NODES) #se eliminan los nodos colapsados\n","\n","  return G,ATTACK,DAMAGE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hlwOczhX27dl"},"source":["def edge_aleatory_cascade_failure(G,Initial_Capacity,Resiliencia,Number_attacks):\n","  \"\"\"\n","  (Graph,float,float,int) -------> (G,Vector con los daños, vector con el número de ataques)\n","  Esta función luego de atacar un enlace aleatoriamente ejecuta el algoritmo de falla en cascada y te entrega como resultado\n","  el grafo atacado, un vector con los daños por ataque y un vector con el número de ataques\n","  \"\"\"\n","  load1 = nx.edge_load_centrality(G)\n","  keys1 = list(load1.keys())\n","  Capacity = []\n","  for i in range(len(keys1)): #se definen condiciones iniciales\n","    q = (1+Initial_Capacity)*load1[keys1[i]]\n","    Capacity.append(q)\n","  DAMAGEEDGE = []\n","  ATTACK = []\n","  for i in range(Number_attacks): #en este for se ejecutan los ataques\n","    NodosBC = len(max(nx.connected_components(G), key = len))\n","    DAMAGEEDGE.append(NodosBC/len(G.nodes()))#registramos el daño con el cambio en el tamaño de la componente principal\n","    ATTACK.append(i) #registramos el número de ataque\n","    edge_remove_aleatory(G,1) #se remueve un edge aleatoriamente\n","    DELETE_EDGES = []\n","    load = nx.edge_load_centrality(G)\n","    for j in G.edges():\n","      val = Resiliencia*Capacity[keys1.index(j)] #calcula el valor de load sobre el cual un nodo colapsara con 100% de probabilidad\n","      if load[j] > val: \n","        DELETE_EDGES.append(j) #enlistamos nodos colapsados\n","      else:\n","        if load[j] > Capacity[keys1.index(j)]: #se elige si un nodo que ha superado su capacidad colapsa\n","          k = st.uniform.rvs()\n","          P = (1/(Resiliencia -1))*(load[j]/Capacity[keys1.index(j)] - 1)\n","          if k <= P:\n","            DELETE_EDGES.append(j)\n","    G.remove_edges_from(DELETE_EDGES) #enlistamos nodos colapsados\n","\n","  return G, ATTACK, DAMAGEEDGE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdV9P7f0onT7"},"source":["# Está función calcula la entropía de distintas distribuciones de medidas para un grafo\n","def graph_entropys(G):\n","  \"\"\"\n","  Recibe un grafo y entrega números\n","  (Graph) -----> Degree_Entropy, Eigenvector_Entropy, Betweenness_Entropy, Closeness_Entropy\n","  \"\"\"\n","  EIGENVECTOR = []\n","  DEGREE = []\n","  BETWEENNESS = []\n","  CLOSENESS = []\n","  LOAD = []\n","  #las siguientes 4 lineas producen 4 diccionarios con los nodos y sus centralidades correspondientes\n","  eigenvector = nx.eigenvector_centrality(G,max_iter = 10000)\n","  degree = nx.degree(G) \n","  betweenness = nx.betweenness_centrality(G) \n","  closeness = nx.closeness_centrality(G) \n","  load = nx.load_centrality(G)\n","  for i in range(len(eigenvector)): #este for desempaqueta los diccionarios, para coger solo las centralidades en listas\n","    EIGENVECTOR.append(eigenvector[i])\n","    DEGREE.append(degree[i])\n","    BETWEENNESS.append(betweenness[i])\n","    CLOSENESS.append(closeness[i])\n","    LOAD.append(load[i])\n","  # aca se usa el modulo stats de scipy para calcular las entropias\n","  Closeness_Entropy = st.entropy(CLOSENESS)\n","  Degree_Entropy = st.entropy(DEGREE)\n","  Eigenvector_Entropy = st.entropy(EIGENVECTOR)\n","  Betweenness_Entropy = st.entropy(BETWEENNESS)\n","  Load_Entropy = st.entropy(LOAD)\n","  return Degree_Entropy, Eigenvector_Entropy, Betweenness_Entropy, Closeness_Entropy, Load_Entropy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj1AD8sBenqA"},"source":["def muestra_ensamble_maxent(muestra,number_of_nodes,P):\n","  \"\"\"\n","  (num_muestra, num_nodes,cum_probab_distribut) -------> (Degree_sequence_prom,adjacencia_insilico)\n","  \"\"\"\n","  connectivity_matrix = np.zeros((muestra, number_of_nodes, number_of_nodes))\n","\n","  for l in range(muestra):\n","      G = maxent_generator(P,number_of_nodes) #generamos un grafo usando el algoritmo de maxima entropia\n","      adjacency = nx.adjacency_matrix(G)\n","      for i in range(number_of_nodes):\n","        for j in range(number_of_nodes):\n","          connectivity_matrix[l][i][j] = adjacency[(i,j)] #guardamos las matrices de adyacencias de los grafos muestreados\n","  adjacencia_insilico = np.zeros((number_of_nodes,number_of_nodes)) \n","\n","  for i in range(number_of_nodes):\n","    for j in range(number_of_nodes):\n","      adjprom = 0\n","      for l in range(muestra):\n","        adjprom = adjprom + connectivity_matrix[l][i][j]\n","      adjprom = adjprom/muestra #promediamos matrices de adyacencia\n","      adjacencia_insilico[i][j] = adjprom \n","  return adjacencia_insilico"],"execution_count":null,"outputs":[]}]}